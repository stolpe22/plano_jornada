# Scraper e Enriquecedor de Plano de Estudos da Jornada de Dados

Este projeto √© uma aplica√ß√£o web constru√≠da com **Streamlit** que automatiza duas tarefas principais:

- **Web Scraping:** Autentica-se na plataforma Jornada de Dados e extrai uma lista completa de todas as trilhas, cursos, m√≥dulos e aulas dispon√≠veis, incluindo o slug para gerar links diretos.
- **Enriquecimento de Dados:** Utiliza um arquivo CSV com um plano de estudos fornecido pelo utilizador e, atrav√©s de correspond√™ncia difusa (fuzzy matching), associa cada m√≥dulo do plano ao seu link correspondente, exportando um novo CSV com os dados enriquecidos.

O objetivo √© transformar um plano de estudos est√°tico numa ferramenta de navega√ß√£o din√¢mica, facilitando o acesso direto ao conte√∫do da plataforma.

---

## ‚ú® Funcionalidades

- **Interface Web Simples:** Uma interface amig√°vel criada com Streamlit que guia o utilizador atrav√©s do processo.
- **Autentica√ß√£o Segura:** O utilizador insere as suas credenciais, que s√£o usadas para criar uma sess√£o autenticada para a raspagem dos dados.
- **Scraping Concorrente:** Utiliza multithreading para acelerar significativamente o processo de busca pelos detalhes das aulas, fazendo m√∫ltiplas requisi√ß√µes em paralelo.
- **Jun√ß√£o Inteligente de Dados:** Emprega a biblioteca thefuzz para fazer a correspond√™ncia entre os nomes dos m√≥dulos no plano de estudos e os nomes extra√≠dos da plataforma, mesmo que n√£o sejam id√™nticos.
- **Exporta√ß√£o de Resultados:** Gera um ficheiro CSV final com o plano de estudos original enriquecido com uma nova coluna contendo os links diretos para a primeira aula de cada m√≥dulo.

---

## üìÇ Estrutura do Projeto

```
/
|
|--- dados/
|    |--- plano_de_estudos.csv              # INPUT: O seu plano de estudos.
|    |--- cursos_jornada.csv                # OUTPUT: Gerado pelo scraper.
|    |--- plano_de_estudos_com_links.csv    # OUTPUT: O resultado final.
|
|--- modules/
|    |--- authenticator.py                  # Lida com a autentica√ß√£o na plataforma.
|    |--- scraper.py                        # Cont√©m toda a l√≥gica de web scraping.
|    |--- data_joiner.py                    # Cont√©m a l√≥gica para juntar os dados.
|
|--- app.py                                # Ficheiro principal da aplica√ß√£o Streamlit.
|
|--- requirements.txt                      # Lista de depend√™ncias do projeto.
```

---

## üöÄ Como Configurar e Executar

Siga os passos abaixo para executar o projeto na sua m√°quina local.

### 1. Pr√©-requisitos

- Python 3.8 ou superior.

### 2. Instala√ß√£o

Clone ou descarregue este reposit√≥rio. Navegue at√© a pasta raiz do projeto e crie um ambiente virtual:

```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar o ambiente virtual (Windows)
.venv\Scripts\activate

# Ativar o ambiente virtual (macOS/Linux)
source .venv/bin/activate
```

Instale as depend√™ncias listadas no requirements.txt:

```bash
pip install -r requirements.txt
```

### 3. Preparar o Plano de Estudos

- Crie a pasta `dados` na raiz do projeto, caso n√£o exista.
- Dentro da pasta `dados`, coloque o seu ficheiro CSV com o plano de estudos e certifique-se de que o nome dele √© `plano_de_estudos.csv`.
- A primeira linha do seu CSV deve ser o cabe√ßalho, contendo pelo menos as colunas **Trilha** e **M√≥dulo**.

### 4. Executar a Aplica√ß√£o

Com o ambiente virtual ativado, execute o seguinte comando no terminal, a partir da pasta raiz do projeto:

```bash
streamlit run app.py
```

O seu navegador abrir√° automaticamente com a interface da aplica√ß√£o.

---

## üìñ Como Usar

A interface √© dividida em dois passos simples:

### 1. Fazer o Scraping

- Insira o seu e-mail e senha da plataforma Jornada de Dados.
- Clique no bot√£o **"Fazer Scraping Agora"**.
- Aguarde o processo terminar. Pode acompanhar o progresso na √°rea de log que aparecer√°. Ao final, ser√° criado o ficheiro `dados/cursos_jornada.csv`.

### 2. Gerar Links no Plano de Estudos

- Ap√≥s o scraping ser conclu√≠do com sucesso, o bot√£o **"Gerar Links no Plano de Estudos"** ser√° habilitado.
- Clique nele para iniciar o processo de correspond√™ncia.
- Ao final, a aplica√ß√£o exibir√° uma amostra do resultado e disponibilizar√° um bot√£o para descarregar o CSV final, `plano_de_estudos_com_links.csv`.

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Streamlit:** Para a cria√ß√£o da interface web.
- **Requests & BeautifulSoup4:** Para a comunica√ß√£o HTTP e parsing do HTML.
- **Pandas:** Para a manipula√ß√£o dos dados e cria√ß√£o dos ficheiros CSV.
- **TheFuzz (FuzzyWuzzy):** Para a l√≥gica de correspond√™ncia de texto difusa.
- **Concurrent.futures:** Para acelerar o scraping atrav√©s de requisi√ß√µes paralelas (multithreading).

---

## üë®‚Äçüíª Contribui√ß√£o

Pull requests s√£o bem-vindos! Sinta-se √† vontade para abrir issues ou sugerir melhorias.

---